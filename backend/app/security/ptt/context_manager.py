"""
MMCODE Security Platform - PTT Context Manager
=============================================

LLM ì»¨í…ìŠ¤íŠ¸ ìµœì í™” ë° ê´€ë¦¬
- ì»¨í…ìŠ¤íŠ¸ ì••ì¶• ë° ìš”ì•½
- ì¤‘ìš” ì •ë³´ ìš°ì„ ìˆœìœ„ ê´€ë¦¬
- í† í° ì œí•œ ëŒ€ì‘

Version: 1.0.0
"""

import json
import logging
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional, Set
from dataclasses import dataclass, field

from ..models import TaskNode, SecurityFinding, PentestPhase
from .task_tree import PTTState

logger = logging.getLogger(__name__)


@dataclass
class ContextPriority:
    """ì»¨í…ìŠ¤íŠ¸ ìš°ì„ ìˆœìœ„ ì„¤ì •"""
    critical_findings: float = 1.0
    recent_tasks: float = 0.8
    available_tasks: float = 0.7
    discovered_assets: float = 0.6
    execution_history: float = 0.5
    statistics: float = 0.3


class PTTContextManager:
    """
    PTT ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ì
    
    LLM ì»¨í…ìŠ¤íŠ¸ ì œí•œ ë¬¸ì œ í•´ê²°:
    - ì¤‘ìš”ë„ ê¸°ë°˜ ì •ë³´ í•„í„°ë§
    - ì ì‘ì  ì••ì¶•
    - ë‹¨ê³„ì  ì„¸ë¶€ì •ë³´ ì œê³µ
    """
    
    def __init__(self, max_context_tokens: int = 4000):
        """
        Args:
            max_context_tokens: ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ í† í° ìˆ˜
        """
        self.max_context_tokens = max_context_tokens
        self.priority_config = ContextPriority()
        
        # ì••ì¶• ë ˆë²¨ë³„ ì„¤ì •
        self.compression_levels = {
            1: {"token_limit": 4000, "detail_level": "full"},
            2: {"token_limit": 3000, "detail_level": "high"}, 
            3: {"token_limit": 2000, "detail_level": "medium"},
            4: {"token_limit": 1000, "detail_level": "low"},
            5: {"token_limit": 500, "detail_level": "minimal"}
        }
    
    def generate_context(
        self,
        ptt_state: PTTState,
        focus_areas: Optional[List[str]] = None,
        max_tokens: Optional[int] = None
    ) -> str:
        """
        PTT ìƒíƒœë¡œë¶€í„° ìµœì í™”ëœ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        
        Args:
            ptt_state: PTT í˜„ì¬ ìƒíƒœ
            focus_areas: ì§‘ì¤‘í•  ì˜ì—­ (findings, tasks, assets ë“±)
            max_tokens: ìµœëŒ€ í† í° ìˆ˜ (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
            
        Returns:
            str: ìµœì í™”ëœ ì»¨í…ìŠ¤íŠ¸
        """
        target_tokens = max_tokens or self.max_context_tokens
        
        # 1ë‹¨ê³„: ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘
        context_sections = self._collect_base_sections(ptt_state)
        
        # 2ë‹¨ê³„: ìš°ì„ ìˆœìœ„ ê¸°ë°˜ í•„í„°ë§
        if focus_areas:
            context_sections = self._filter_by_focus(context_sections, focus_areas)
        
        # 3ë‹¨ê³„: í† í° ì œí•œì— ë§ì¶° ì••ì¶•
        context = self._compress_to_limit(context_sections, target_tokens)
        
        return context
    
    def _collect_base_sections(self, state: PTTState) -> Dict[str, Dict]:
        """ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ ì„¹ì…˜ ìˆ˜ì§‘"""
        sections = {}
        
        # í—¤ë” ì •ë³´
        sections["header"] = {
            "content": self._generate_header(state),
            "priority": 1.0,
            "estimated_tokens": 100
        }
        
        # ì¤‘ìš”í•œ ë°œê²¬ì‚¬í•­
        critical_findings = [
            f for f in state.findings 
            if f.severity.value in ['critical', 'high']
        ]
        if critical_findings:
            sections["critical_findings"] = {
                "content": self._format_critical_findings(critical_findings),
                "priority": self.priority_config.critical_findings,
                "estimated_tokens": len(critical_findings) * 80
            }
        
        # ìµœê·¼ ì™„ë£Œ ì‘ì—…
        recent_completions = [
            task for task in state.all_nodes.values()
            if (task.status == "completed" and 
                task.completed_at and
                task.completed_at > datetime.utcnow() - timedelta(hours=2))
        ]
        if recent_completions:
            sections["recent_tasks"] = {
                "content": self._format_recent_tasks(recent_completions),
                "priority": self.priority_config.recent_tasks,
                "estimated_tokens": len(recent_completions) * 60
            }
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ë‹¤ìŒ ì‘ì—…
        available_tasks = [
            task for task in state.all_nodes.values()
            if task.status == "available"
        ]
        if available_tasks:
            sections["available_tasks"] = {
                "content": self._format_available_tasks(available_tasks),
                "priority": self.priority_config.available_tasks,
                "estimated_tokens": len(available_tasks) * 70
            }
        
        # ë°œê²¬ëœ ìì‚°
        if state.discovered_assets:
            sections["discovered_assets"] = {
                "content": self._format_discovered_assets(state.discovered_assets),
                "priority": self.priority_config.discovered_assets,
                "estimated_tokens": len(state.discovered_assets) * 20
            }
        
        # ì‹¤í–‰ í†µê³„
        sections["statistics"] = {
            "content": self._format_statistics(state),
            "priority": self.priority_config.statistics,
            "estimated_tokens": 150
        }
        
        return sections
    
    def _generate_header(self, state: PTTState) -> str:
        """í—¤ë” ì •ë³´ ìƒì„±"""
        total_tasks = len(state.all_nodes)
        completed_tasks = len(state.completed_tasks)
        completion_rate = (completed_tasks / total_tasks * 100) if total_tasks > 0 else 0
        
        header = [
            f"ğŸ¯ **Pentesting Task Tree: {state.engagement_scope.engagement_name}**",
            f"ğŸ“Š Progress: {completed_tasks}/{total_tasks} tasks ({completion_rate:.1f}%)",
            f"ğŸ” Findings: {len(state.findings)} total"
        ]
        
        if state.findings:
            critical_count = len([f for f in state.findings if f.severity.value == 'critical'])
            high_count = len([f for f in state.findings if f.severity.value == 'high'])
            if critical_count > 0 or high_count > 0:
                header.append(f"âš ï¸ High-risk findings: {critical_count} critical, {high_count} high")
        
        return "\n".join(header)
    
    def _format_critical_findings(self, findings: List[SecurityFinding]) -> str:
        """ì¤‘ìš”í•œ ë°œê²¬ì‚¬í•­ í¬ë§·íŒ…"""
        if not findings:
            return ""
        
        lines = ["## ğŸš¨ Critical Findings"]
        
        for finding in findings[:5]:  # ìµœëŒ€ 5ê°œ
            severity_icon = "ğŸ”´" if finding.severity.value == 'critical' else "ğŸŸ "
            lines.append(
                f"{severity_icon} **{finding.title}** "
                f"({finding.severity.value.upper()})"
            )
            if finding.affected_asset:
                lines.append(f"   ğŸ“ Target: {finding.affected_asset}")
            if finding.cvss_score:
                lines.append(f"   ğŸ“Š CVSS: {finding.cvss_score}")
        
        if len(findings) > 5:
            lines.append(f"... and {len(findings) - 5} more critical findings")
        
        return "\n".join(lines)
    
    def _format_recent_tasks(self, tasks: List[TaskNode]) -> str:
        """ìµœê·¼ ì™„ë£Œ ì‘ì—… í¬ë§·íŒ…"""
        if not tasks:
            return ""
        
        lines = ["## âœ… Recently Completed"]
        
        # ì‹œê°„ ìˆœ ì •ë ¬
        sorted_tasks = sorted(
            tasks, 
            key=lambda t: t.completed_at or datetime.min,
            reverse=True
        )
        
        for task in sorted_tasks[:4]:  # ìµœëŒ€ 4ê°œ
            phase_icon = self._get_phase_icon(task.phase)
            lines.append(f"{phase_icon} {task.name}")
            if task.findings:
                lines.append(f"   ğŸ” Findings: {len(task.findings)}")
        
        return "\n".join(lines)
    
    def _format_available_tasks(self, tasks: List[TaskNode]) -> str:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ì‘ì—… í¬ë§·íŒ…"""
        if not tasks:
            return ""
        
        lines = ["## ğŸ“‹ Available Tasks"]
        
        # ìš°ì„ ìˆœìœ„ ìˆœ ì •ë ¬
        sorted_tasks = sorted(
            tasks,
            key=lambda t: t.priority_score,
            reverse=True
        )
        
        for task in sorted_tasks[:6]:  # ìµœëŒ€ 6ê°œ
            phase_icon = self._get_phase_icon(task.phase)
            priority = "ğŸ”¥" if task.priority_score > 0.8 else "ğŸ“Œ"
            
            lines.append(
                f"{priority} {task.name} "
                f"({task.phase.value}, priority: {task.priority_score:.1f})"
            )
            
            if task.requires_approval:
                lines.append("   âš ï¸ Requires approval")
            
            if task.estimated_duration_seconds:
                duration_min = task.estimated_duration_seconds // 60
                lines.append(f"   â±ï¸ Est: {duration_min}min")
        
        return "\n".join(lines)
    
    def _format_discovered_assets(self, assets: Set[str]) -> str:
        """ë°œê²¬ëœ ìì‚° í¬ë§·íŒ…"""
        if not assets:
            return ""
        
        lines = ["## ğŸŒ Discovered Assets"]
        
        asset_list = list(assets)
        
        # IPì™€ ë„ë©”ì¸ ë¶„ë¦¬
        ips = [a for a in asset_list if self._is_ip(a)]
        domains = [a for a in asset_list if not self._is_ip(a)]
        
        if ips:
            lines.append(f"ğŸ“ IPs: {', '.join(ips[:5])}")
            if len(ips) > 5:
                lines.append(f"   ... and {len(ips) - 5} more")
        
        if domains:
            lines.append(f"ğŸŒ Domains: {', '.join(domains[:5])}")
            if len(domains) > 5:
                lines.append(f"   ... and {len(domains) - 5} more")
        
        return "\n".join(lines)
    
    def _format_statistics(self, state: PTTState) -> str:
        """í†µê³„ ì •ë³´ í¬ë§·íŒ…"""
        total_tasks = len(state.all_nodes)
        completed = len(state.completed_tasks)
        failed = len(state.failed_tasks)
        
        # í˜ì´ì¦ˆë³„ ë¶„í¬
        phase_counts = {}
        for task in state.all_nodes.values():
            phase = task.phase.value
            phase_counts[phase] = phase_counts.get(phase, 0) + 1
        
        lines = [
            "## ğŸ“Š Statistics",
            f"Tasks: {completed} completed, {failed} failed, {total_tasks - completed - failed} pending"
        ]
        
        if phase_counts:
            phase_summary = ", ".join([
                f"{phase}: {count}" for phase, count in sorted(phase_counts.items())
            ])
            lines.append(f"Phases: {phase_summary}")
        
        return "\n".join(lines)
    
    def _get_phase_icon(self, phase: PentestPhase) -> str:
        """í˜ì´ì¦ˆë³„ ì•„ì´ì½˜"""
        icons = {
            PentestPhase.RECONNAISSANCE: "ğŸ”",
            PentestPhase.SCANNING: "ğŸ“¡",
            PentestPhase.ENUMERATION: "ğŸ“‹",
            PentestPhase.VULNERABILITY_ASSESSMENT: "ğŸ”",
            PentestPhase.EXPLOITATION: "ğŸ’¥",
            PentestPhase.POST_EXPLOITATION: "ğŸ¯",
            PentestPhase.REPORTING: "ğŸ“„"
        }
        return icons.get(phase, "ğŸ“Œ")
    
    def _is_ip(self, value: str) -> bool:
        """IP ì£¼ì†Œ ì—¬ë¶€ í™•ì¸"""
        try:
            import ipaddress
            ipaddress.ip_address(value)
            return True
        except ValueError:
            return False
    
    def _filter_by_focus(
        self,
        sections: Dict[str, Dict],
        focus_areas: List[str]
    ) -> Dict[str, Dict]:
        """ì§‘ì¤‘ ì˜ì—­ì— ë”°ë¥¸ í•„í„°ë§"""
        if not focus_areas:
            return sections
        
        # ì§‘ì¤‘ ì˜ì—­ë³„ ìš°ì„ ìˆœìœ„ ë¶€ìŠ¤íŠ¸
        focus_boost = {
            "findings": ["critical_findings"],
            "tasks": ["available_tasks", "recent_tasks"],
            "assets": ["discovered_assets"],
            "stats": ["statistics"]
        }
        
        for focus in focus_areas:
            if focus in focus_boost:
                for section_name in focus_boost[focus]:
                    if section_name in sections:
                        sections[section_name]["priority"] *= 1.5
        
        return sections
    
    def _compress_to_limit(
        self,
        sections: Dict[str, Dict],
        target_tokens: int
    ) -> str:
        """í† í° ì œí•œì— ë§ì¶° ì••ì¶•"""
        # ìš°ì„ ìˆœìœ„ ìˆœ ì •ë ¬
        sorted_sections = sorted(
            sections.items(),
            key=lambda x: x[1]["priority"],
            reverse=True
        )
        
        result_parts = []
        used_tokens = 0
        
        for section_name, section_data in sorted_sections:
            estimated_tokens = section_data["estimated_tokens"]
            
            if used_tokens + estimated_tokens <= target_tokens:
                # ì „ì²´ ì„¹ì…˜ í¬í•¨
                result_parts.append(section_data["content"])
                used_tokens += estimated_tokens
            else:
                # ë‚¨ì€ ê³µê°„ì— ë§ì¶° ì••ì¶•
                remaining_tokens = target_tokens - used_tokens
                if remaining_tokens > 100:  # ìµœì†Œí•œì˜ ìœ ìš©í•œ ì •ë³´
                    compressed = self._compress_section(
                        section_data["content"],
                        remaining_tokens
                    )
                    if compressed:
                        result_parts.append(compressed)
                        used_tokens = target_tokens
                break
        
        context = "\n\n".join(result_parts)
        
        # í† í° ì‚¬ìš©ëŸ‰ ë¡œê¹…
        logger.info(f"Generated PTT context: ~{used_tokens} tokens, {len(result_parts)} sections")
        
        return context
    
    def _compress_section(self, content: str, max_tokens: int) -> Optional[str]:
        """ì„¹ì…˜ ì••ì¶•"""
        lines = content.split('\n')
        
        if not lines:
            return None
        
        # í—¤ë”ëŠ” ìœ ì§€
        header = lines[0] if lines[0].startswith('#') else ""
        content_lines = lines[1:] if header else lines
        
        # í† í° ì¶”ì • (1 token â‰ˆ 4 characters)
        chars_per_token = 4
        max_chars = max_tokens * chars_per_token
        
        if header:
            max_chars -= len(header) + 2  # í—¤ë” + ì¤„ë°”ê¿ˆ
        
        result_lines = []
        current_chars = 0
        
        for line in content_lines:
            if current_chars + len(line) + 1 <= max_chars:
                result_lines.append(line)
                current_chars += len(line) + 1
            else:
                if result_lines:
                    result_lines.append("... (truncated)")
                break
        
        if header:
            return header + "\n" + "\n".join(result_lines)
        else:
            return "\n".join(result_lines)
    
    def get_compression_stats(self) -> Dict[str, Any]:
        """ì••ì¶• í†µê³„ ë°˜í™˜"""
        return {
            "max_context_tokens": self.max_context_tokens,
            "compression_levels": len(self.compression_levels),
            "priority_config": {
                "critical_findings": self.priority_config.critical_findings,
                "recent_tasks": self.priority_config.recent_tasks,
                "available_tasks": self.priority_config.available_tasks,
                "discovered_assets": self.priority_config.discovered_assets,
                "statistics": self.priority_config.statistics
            }
        }